{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Promesse7/Prom.AI/blob/main/Individual-Contest/Pixel/Pixel_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441c46a0-b20a-4dc1-9227-f659833a7d2f",
      "metadata": {
        "id": "441c46a0-b20a-4dc1-9227-f659833a7d2f"
      },
      "source": [
        "# Pixel Efficiency: Reference Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5d573331",
      "metadata": {
        "id": "5d573331"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers huggingface_hub accelerate pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "fbaa9657e2444c5a88b10ca85411868b",
            "c4a1544ce27841a3a648c5b58d6f9ab1",
            "f8ea46fec3b848d4bc1ed271380b4212",
            "ffb05799106949c4a9b75fc4f1bb3e73",
            "8d9d1ee67901447c99b4f6c90c279880",
            "cb28ffc900d4416691fc1ade434b0622",
            "7f9b3d56da98483894f97957cfdf1012",
            "99015ad0ff6c4bfbae0645d6822bc206",
            "b36b28c791984ad29ce1ae26eabb17f2",
            "1867f60c553e4091938eb27cbc7b087c",
            "b2f014664d5b43db8f571bdeb53614c1",
            "3d45f7b95eef49738822743876b4dbfc",
            "741e99269f024bcabc9fcb758acc7c73",
            "289693679d8c45f19cc4c14af57a12a9",
            "5291ff294c2e46719e7cf7ad8da30fe4",
            "a0543e7d64de4d3c8289e50bb297c874",
            "a9d4c76566404b788ebf36fa528b97c0",
            "3d64d55a004e4854ac6b46137ccdccc2",
            "abb6b0b3477d45bd9cf0259258eaedf3",
            "ac0d8a36c8594ee899276f1d3be44e13"
          ]
        },
        "id": "fIYeZLpdIXK3",
        "outputId": "b6800ba8-57b7-43d9-ab1f-2249d5d68e30"
      },
      "id": "fIYeZLpdIXK3",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbaa9657e2444c5a88b10ca85411868b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 4 — Configuration**"
      ],
      "metadata": {
        "id": "DVfgyOSIJWcD"
      },
      "id": "DVfgyOSIJWcD"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from typing import Optional\n",
        "from transformers.models.clip.modeling_clip import CLIPVisionTransformer, CLIPVisionConfig, BaseModelOutputWithPooling\n",
        "\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_PATH = \"IOAI-official/IOAI-2025-Pixel-test\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_PATH = \"openai/clip-vit-large-patch14\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BACKGROUND_CLASS = \"other\"\n",
        "\n",
        "# Image and Masking Configuration\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "RETAIN_RATIO = 0.0625\n",
        "MEAN_COLOR = (0, 0, 0)\n",
        "STRIDE = 2\n",
        "TOP_K = 3\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(DATASET_PATH, split=SPLIT)\n",
        "\n",
        "print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF2wHJLvPwAQ",
        "outputId": "52677467-dd46-41b1-a558-508bd226624b"
      },
      "id": "wF2wHJLvPwAQ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset loaded successfully! Total samples: 698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_all_rectangular_regions(image_size=224, patch_size=14, retain_ratio=RETAIN_RATIO, max_aspect_ratio=1.2, stride=1):\n",
        "    \"\"\"Generate rectangular regions with optimizations for speed\"\"\"\n",
        "    max_pixels = int(retain_ratio * image_size * image_size)\n",
        "    patches_per_side = image_size // patch_size\n",
        "    patch_area = patch_size * patch_size\n",
        "    target_patches = max_pixels // patch_area\n",
        "\n",
        "    min_patches = max(1, target_patches - 1)\n",
        "    max_patches = target_patches + 1\n",
        "\n",
        "    regions = []\n",
        "    region_to_patches = []\n",
        "\n",
        "    # Pre-compute valid rectangle dimensions\n",
        "    valid_dims = []\n",
        "    for width_patches in range(1, patches_per_side + 1):\n",
        "        for height_patches in range(1, patches_per_side + 1):\n",
        "            total_patches = width_patches * height_patches\n",
        "            if min_patches <= total_patches <= max_patches:\n",
        "                aspect_ratio = max(width_patches, height_patches) / min(width_patches, height_patches)\n",
        "                if aspect_ratio <= max_aspect_ratio:\n",
        "                    valid_dims.append((width_patches, height_patches, total_patches))\n",
        "\n",
        "    # Generate rectangles using stride for positions\n",
        "    for width_patches, height_patches, total_patches in valid_dims:\n",
        "        for top_patch in range(0, patches_per_side - height_patches + 1, stride):\n",
        "            for left_patch in range(0, patches_per_side - width_patches + 1, stride):\n",
        "                bottom_patch = top_patch + height_patches\n",
        "                right_patch = left_patch + width_patches\n",
        "\n",
        "                pixel_coords = (\n",
        "                    top_patch * patch_size,\n",
        "                    left_patch * patch_size,\n",
        "                    bottom_patch * patch_size,\n",
        "                    right_patch * patch_size\n",
        "                )\n",
        "                regions.append(pixel_coords)\n",
        "\n",
        "                covered_patches = []\n",
        "                for p_row in range(top_patch, bottom_patch):\n",
        "                    for p_col in range(left_patch, right_patch):\n",
        "                        patch_idx = p_row * patches_per_side + p_col\n",
        "                        covered_patches.append(patch_idx)\n",
        "                region_to_patches.append(covered_patches)\n",
        "\n",
        "    return regions, region_to_patches\n",
        "\n",
        "\n",
        "class MaskCLIPVisionTransformer(CLIPVisionTransformer):\n",
        "    \"\"\"Modified CLIP Vision Transformer that supports mask tokens for all possible rectangular regions\"\"\"\n",
        "\n",
        "    def __init__(self, config: CLIPVisionConfig, retain_ratio=RETAIN_RATIO):\n",
        "        super().__init__(config)\n",
        "        self.retain_ratio = retain_ratio\n",
        "        self.num_patches = (config.image_size // config.patch_size) ** 2\n",
        "\n",
        "        self.regions, self.region_to_patches = generate_all_rectangular_regions(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            retain_ratio=retain_ratio,\n",
        "            max_aspect_ratio=1.2,\n",
        "            stride=STRIDE\n",
        "        )\n",
        "        self.num_mask_tokens = len(self.regions)\n",
        "\n",
        "        self.mask_tokens = nn.Parameter(torch.randn(1, self.num_mask_tokens, config.hidden_size))\n",
        "\n",
        "    def create_mask_attention_matrix(self, batch_size):\n",
        "        \"\"\"Create attention mask matrix for all rectangular regions\"\"\"\n",
        "        N = self.num_patches\n",
        "        M = self.num_mask_tokens\n",
        "        total_tokens = N + 1 + M\n",
        "\n",
        "        attention_mask = torch.zeros(total_tokens, total_tokens, dtype=torch.bool, device=self.mask_tokens.device)\n",
        "\n",
        "        # Class token and image patches do NOT attend to mask tokens\n",
        "        attention_mask[:N+1, N+1:] = True\n",
        "\n",
        "        # Each mask token attends to its specific image patches (not CLS)\n",
        "        attention_mask[N+1:, 1:N+1] = True\n",
        "\n",
        "        # Then allow each mask token to attend to its assigned patches\n",
        "        for mask_idx in range(M):\n",
        "            covered_patches = self.region_to_patches[mask_idx]\n",
        "            for patch_idx in covered_patches:\n",
        "                token_pos = 1 + patch_idx\n",
        "                attention_mask[N + 1 + mask_idx, token_pos] = False\n",
        "\n",
        "        # Mask tokens do NOT attend to each other\n",
        "        attention_mask[N+1:, N+1:] = True\n",
        "        # Allow self-attention for each mask token\n",
        "        for i in range(M):\n",
        "            attention_mask[N + 1 + i, N + 1 + i] = False\n",
        "\n",
        "        return attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        interpolate_pos_encoding: Optional[bool] = False,\n",
        "        use_mask_tokens: bool = False,\n",
        "    ) -> BaseModelOutputWithPooling:\n",
        "        \"\"\"Forward pass with optional mask tokens\"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Get embeddings (patches + class token)\n",
        "        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
        "        hidden_states = self.pre_layrnorm(hidden_states)\n",
        "\n",
        "        if use_mask_tokens:\n",
        "            # Add mask tokens to the sequence\n",
        "            batch_size = hidden_states.shape[0]\n",
        "\n",
        "            cls_token_embedding = hidden_states[:, 0:1, :]\n",
        "            mask_tokens_expanded = cls_token_embedding.expand(batch_size, self.num_mask_tokens, -1)\n",
        "\n",
        "            if mask_tokens_expanded.device != hidden_states.device:\n",
        "                mask_tokens_expanded = mask_tokens_expanded.to(hidden_states.device)\n",
        "\n",
        "            hidden_states = torch.cat([hidden_states, mask_tokens_expanded], dim=1)\n",
        "\n",
        "            # Create custom attention mask\n",
        "            attention_mask = self.create_mask_attention_matrix(batch_size)\n",
        "\n",
        "            seq_len = hidden_states.shape[1]\n",
        "            attention_mask_4d = attention_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1, -1)\n",
        "            attention_mask_4d = attention_mask_4d.float()\n",
        "            attention_mask_4d = attention_mask_4d.masked_fill(attention_mask_4d == 1, float('-inf'))\n",
        "            attention_mask_4d = attention_mask_4d.masked_fill(attention_mask_4d == 0, 0.0)\n",
        "        else:\n",
        "            attention_mask_4d = None\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_outputs = self.encoder(\n",
        "            inputs_embeds=hidden_states,\n",
        "            attention_mask=attention_mask_4d,\n",
        "            causal_attention_mask=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = encoder_outputs.last_hidden_state\n",
        "\n",
        "        if use_mask_tokens:\n",
        "            # Extract different token types\n",
        "            class_token_output = last_hidden_state[:, 0]\n",
        "            mask_tokens_output = last_hidden_state[:, self.num_patches + 1:]\n",
        "\n",
        "            # Apply post layer norm\n",
        "            pooled_output = self.post_layernorm(class_token_output)\n",
        "            mask_tokens_output = self.post_layernorm(mask_tokens_output)\n",
        "\n",
        "            return {\n",
        "                'last_hidden_state': last_hidden_state,\n",
        "                'pooler_output': pooled_output,\n",
        "                'mask_tokens_output': mask_tokens_output,\n",
        "                'hidden_states': encoder_outputs.hidden_states,\n",
        "                'attentions': encoder_outputs.attentions,\n",
        "            }\n",
        "        else:\n",
        "            # Standard CLIP behavior\n",
        "            pooled_output = last_hidden_state[:, 0, :]\n",
        "            pooled_output = self.post_layernorm(pooled_output)\n",
        "\n",
        "            return BaseModelOutputWithPooling(\n",
        "                last_hidden_state=last_hidden_state,\n",
        "                pooler_output=pooled_output,\n",
        "                hidden_states=encoder_outputs.hidden_states,\n",
        "                attentions=encoder_outputs.attentions,\n",
        "            )\n",
        "\n",
        "\n",
        "def apply_mask_with_mean(image, mask, mean_rgb=MEAN_COLOR):\n",
        "    \"\"\"Apply arbitrary binary mask to image, replacing masked areas with mean values\"\"\"\n",
        "    img_array = np.array(image).copy()\n",
        "\n",
        "    if isinstance(mask, Image.Image):\n",
        "        mask_array = np.array(mask.convert('L')) > 127\n",
        "    else:\n",
        "        mask_array = mask > 0\n",
        "\n",
        "    mask_3d = np.stack([mask_array] * 3, axis=2)\n",
        "    mean_values = np.array([int(m * 255) for m in mean_rgb])\n",
        "    img_array = np.where(mask_3d, img_array, mean_values.reshape(1, 1, 3))\n",
        "\n",
        "    return Image.fromarray(img_array.astype(np.uint8))\n",
        "\n",
        "\n",
        "def compute_vision_features_once(model, image, mask_vision_model):\n",
        "    \"\"\"\n",
        "    Compute vision features once for efficient reuse across multiple mask selection functions.\n",
        "    This eliminates redundant forward passes.\n",
        "    \"\"\"\n",
        "    image_inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vision_outputs = mask_vision_model(\n",
        "            pixel_values=image_inputs['pixel_values'],\n",
        "            use_mask_tokens=True\n",
        "        )\n",
        "\n",
        "        full_image_features = vision_outputs['pooler_output']\n",
        "        if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "            full_image_features = model.visual_projection(full_image_features)\n",
        "\n",
        "        mask_tokens_features = vision_outputs['mask_tokens_output']\n",
        "        if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "            batch_size, num_tokens, embed_dim = mask_tokens_features.shape\n",
        "            mask_tokens_features = mask_tokens_features.view(-1, embed_dim)\n",
        "            mask_tokens_features = model.visual_projection(mask_tokens_features)\n",
        "            mask_tokens_features = mask_tokens_features.view(batch_size, num_tokens, -1)\n",
        "\n",
        "        full_image_features = full_image_features / full_image_features.norm(dim=-1, keepdim=True)\n",
        "        mask_tokens_features = mask_tokens_features / mask_tokens_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return vision_outputs, full_image_features, mask_tokens_features\n",
        "\n",
        "\n",
        "def find_best_mask_region_calibrated(model, image, class_names, mask_vision_model, text_features,\n",
        "                                   vision_outputs=None, full_image_features=None, mask_tokens_features=None,\n",
        "                                   return_detailed=False):\n",
        "    \"\"\"Find the best mask region using MaskCLIP approach with calibration for black-pixel masking\"\"\"\n",
        "    num_mask_tokens = mask_vision_model.num_mask_tokens\n",
        "\n",
        "    # Use pre-computed features if provided, otherwise compute them\n",
        "    if vision_outputs is None or full_image_features is None or mask_tokens_features is None:\n",
        "        image_inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vision_outputs = mask_vision_model(\n",
        "                pixel_values=image_inputs['pixel_values'],\n",
        "                use_mask_tokens=True\n",
        "            )\n",
        "\n",
        "            full_image_features = vision_outputs['pooler_output']\n",
        "            if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "                full_image_features = model.visual_projection(full_image_features)\n",
        "\n",
        "            mask_tokens_features = vision_outputs['mask_tokens_output']\n",
        "            if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "                batch_size, num_tokens, embed_dim = mask_tokens_features.shape\n",
        "                mask_tokens_features = mask_tokens_features.view(-1, embed_dim)\n",
        "                mask_tokens_features = model.visual_projection(mask_tokens_features)\n",
        "                mask_tokens_features = mask_tokens_features.view(batch_size, num_tokens, -1)\n",
        "\n",
        "            full_image_features = full_image_features / full_image_features.norm(dim=-1, keepdim=True)\n",
        "            mask_tokens_features = mask_tokens_features / mask_tokens_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute similarity between full image and text\n",
        "    full_image_similarities = torch.matmul(full_image_features, text_features.T)\n",
        "    full_image_prediction = torch.argmax(full_image_similarities, dim=-1)\n",
        "    predicted_class_idx = full_image_prediction.item()\n",
        "\n",
        "    # Compute similarities for each mask token\n",
        "    mask_similarities = torch.matmul(mask_tokens_features.squeeze(0), text_features.T)\n",
        "    mask_predictions = torch.argmax(mask_similarities, dim=-1)\n",
        "\n",
        "    # Get candidates that predict the same class as full image, sorted by confidence\n",
        "    matching_masks = (mask_predictions == predicted_class_idx)\n",
        "\n",
        "    if matching_masks.any():\n",
        "        candidate_indices = torch.where(matching_masks)[0]\n",
        "        candidate_confidences = mask_similarities[candidate_indices, predicted_class_idx]\n",
        "        sorted_indices = torch.argsort(candidate_confidences, descending=True)\n",
        "        sorted_candidates = candidate_indices[sorted_indices]\n",
        "    else:\n",
        "        # If no exact matches, use all candidates sorted by confidence for predicted class\n",
        "        candidate_confidences = mask_similarities[:, predicted_class_idx]\n",
        "        sorted_candidates = torch.topk(candidate_confidences, len(candidate_confidences)).indices\n",
        "\n",
        "    # OPTIMIZATION: If TOP_K=1, skip calibration and return best candidate directly\n",
        "    if TOP_K == 1:\n",
        "        return sorted_candidates[0].item()\n",
        "\n",
        "    # CALIBRATION STEP: Test top K candidates, return immediately when one is correct\n",
        "    calibration_results = []\n",
        "    candidates_to_test = sorted_candidates[:TOP_K]\n",
        "\n",
        "    for i, candidate_idx in enumerate(candidates_to_test):\n",
        "        candidate_idx_item = candidate_idx.item()\n",
        "\n",
        "        # Create masked image for this candidate\n",
        "        coordinates = mask_idx_to_coordinates(candidate_idx_item, mask_vision_model)\n",
        "        mask = generate_mask_from_coordinates(image, coordinates)\n",
        "        masked_image = apply_mask_with_mean(image, mask)\n",
        "\n",
        "        # Test with actual forward pass\n",
        "        with torch.no_grad():\n",
        "            masked_image_inputs = processor(images=masked_image, return_tensors=\"pt\").to(DEVICE)\n",
        "            masked_image_features = model.get_image_features(**masked_image_inputs)\n",
        "            masked_image_features = masked_image_features / masked_image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            masked_similarities = torch.matmul(masked_image_features, text_features.T)\n",
        "            masked_prediction = torch.argmax(masked_similarities, dim=-1).item()\n",
        "            masked_confidence = masked_similarities[0, predicted_class_idx].item()\n",
        "\n",
        "        # If this candidate predicts correctly, return it immediately (early exit optimization)\n",
        "        if masked_prediction == predicted_class_idx:\n",
        "            return candidate_idx_item\n",
        "\n",
        "        # Store failed calibration result\n",
        "        calibration_results.append((candidate_idx_item, masked_confidence))\n",
        "\n",
        "    # If we reach here, all TOP_K candidates failed calibration\n",
        "    # Fall back to the next best candidate from sorted list WITHOUT additional calibration\n",
        "    if len(sorted_candidates) > TOP_K:\n",
        "        return sorted_candidates[TOP_K].item()\n",
        "    else:\n",
        "        # If no more candidates available, return the best failed calibration result\n",
        "        if calibration_results:\n",
        "            return max(calibration_results, key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            # Ultimate fallback: return the best mask token prediction\n",
        "            return sorted_candidates[0].item()\n",
        "\n",
        "\n",
        "def mask_idx_to_coordinates(mask_idx, mask_vision_model):\n",
        "    \"\"\"Convert mask token index to image coordinates using the pre-computed regions\"\"\"\n",
        "    if mask_idx >= len(mask_vision_model.regions):\n",
        "        raise ValueError(f\"mask_idx {mask_idx} is out of range. Only {len(mask_vision_model.regions)} regions available.\")\n",
        "\n",
        "    top, left, bottom, right = mask_vision_model.regions[mask_idx]\n",
        "    return ((top, left), (bottom, right))\n",
        "\n",
        "\n",
        "def generate_mask_from_coordinates(image, coordinates):\n",
        "    \"\"\"Generate a binary mask from crop coordinates\"\"\"\n",
        "    H, W = 224, 224\n",
        "    mask = np.zeros((H, W), dtype=np.int8)\n",
        "\n",
        "    (top, left), (bottom, right) = coordinates\n",
        "    mask[top:bottom, left:right] = 1\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# Create the MaskCLIP model\n",
        "print(\"Creating MaskCLIP model...\")\n",
        "mask_vision_model = MaskCLIPVisionTransformer(model.vision_model.config, retain_ratio=RETAIN_RATIO)\n",
        "mask_vision_model.load_state_dict(model.vision_model.state_dict(), strict=False)\n",
        "mask_vision_model = mask_vision_model.to(DEVICE)\n",
        "mask_vision_model.eval()\n",
        "print(\"MaskCLIP model created successfully.\")\n",
        "\n",
        "# Get class names from training dataset for consistent evaluation\n",
        "train_dataset = load_dataset(\"IOAI-official/IOAI-2025-Pixel-train\", split=\"train\")\n",
        "class_names_eval = list(set([item['name'] for item in train_dataset])) + [BACKGROUND_CLASS]\n",
        "\n",
        "# Prepare text features once for efficiency\n",
        "print(\"Preparing text features...\")\n",
        "text_inputs_eval = processor(text=class_names_eval, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    text_features_eval = model.get_text_features(**text_inputs_eval)\n",
        "    text_features_eval = text_features_eval / text_features_eval.norm(dim=-1, keepdim=True)\n",
        "print(\"Text features prepared.\")\n",
        "\n",
        "# Main evaluation loop\n",
        "masks = {}\n",
        "total_correct = 0\n",
        "total_processed = 0\n",
        "\n",
        "for item in tqdm(dataset):\n",
        "    image = item['image']\n",
        "    total_processed += 1\n",
        "\n",
        "    try:\n",
        "        # Compute vision features once for efficiency (eliminates redundant forward passes)\n",
        "        vision_outputs, full_image_features, mask_tokens_features = compute_vision_features_once(\n",
        "            model, image, mask_vision_model\n",
        "        )\n",
        "\n",
        "        # Get prediction from pre-computed features\n",
        "        full_image_similarities = torch.matmul(full_image_features, text_features_eval.T)\n",
        "        predicted_class_idx = torch.argmax(full_image_similarities, dim=-1).item()\n",
        "\n",
        "        best_mask_idx = find_best_mask_region_calibrated(\n",
        "            model, image, class_names_eval, mask_vision_model, text_features_eval,\n",
        "            vision_outputs=vision_outputs, full_image_features=full_image_features,\n",
        "            mask_tokens_features=mask_tokens_features\n",
        "        )\n",
        "\n",
        "        coordinates = mask_idx_to_coordinates(best_mask_idx, mask_vision_model)\n",
        "\n",
        "        # Validate the mask\n",
        "        mask = generate_mask_from_coordinates(image, coordinates)\n",
        "        assert mask.shape == (224, 224), \"Mask should be 224x224\"\n",
        "        assert mask.sum() <= RETAIN_RATIO * 224 * 224, \"You should leave only 6.25% of pixels\"\n",
        "\n",
        "\n",
        "        # Save the coordinates\n",
        "        idx = item['idx']\n",
        "        masks[idx] = coordinates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {item['idx']}: {e}\")\n",
        "        # Fallback to a small center region if there's an error\n",
        "        if len(mask_vision_model.regions) > 0:\n",
        "            region_sizes = [(r[2]-r[0])*(r[3]-r[1]) for r in mask_vision_model.regions]\n",
        "            min_region_idx = region_sizes.index(min(region_sizes))\n",
        "            fallback_coords = mask_idx_to_coordinates(min_region_idx, mask_vision_model)\n",
        "        else:\n",
        "            fallback_coords = ((84, 84), (140, 140))\n",
        "        masks[item['idx']] = fallback_coords\n",
        "\n",
        "# Save as JSONL (one JSON object per line) - much safer than pickle\n",
        "with open('submission.jsonl', 'w') as f:\n",
        "    for idx, coordinates in masks.items():\n",
        "        json.dump({\"idx\": idx, \"coordinates\": coordinates}, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"Masks saved to masks.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dupVS23MOyBQ",
        "outputId": "b96e0bd4-d30b-4a1b-ef64-1a07b6568c4d"
      },
      "id": "dupVS23MOyBQ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating MaskCLIP model...\n",
            "MaskCLIP model created successfully.\n",
            "Preparing text features...\n",
            "Text features prepared.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 698/698 [02:45<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masks saved to masks.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python metrics.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJAmRU72NbA1",
        "outputId": "a5397759-0b4b-4ad5-8277-51ac8cedf55d"
      },
      "id": "kJAmRU72NbA1",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-26 01:27:30.136442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766712450.167087    9897 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766712450.176494    9897 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766712450.198428    9897 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766712450.198460    9897 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766712450.198475    9897 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766712450.198479    9897 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Valid coordinates: 698/698\n",
            "Loading CLIP model and processor: openai/clip-vit-large-patch14...\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Model and processor loaded successfully.\n",
            "Class 'dog': 214 total, 64 to A, 150 to B\n",
            "Class 'rabbit': 99 total, 29 to A, 70 to B\n",
            "Class 'cat': 56 total, 16 to A, 40 to B\n",
            "Class 'lynx': 172 total, 51 to A, 121 to B\n",
            "Class 'bird': 58 total, 17 to A, 41 to B\n",
            "Class 'skunk': 21 total, 6 to A, 15 to B\n",
            "Class 'deer': 45 total, 13 to A, 32 to B\n",
            "Class 'opossum': 16 total, 4 to A, 12 to B\n",
            "Class 'squirrel': 17 total, 5 to A, 12 to B\n",
            "Stratified split: Set A has 205 items, Set B has 493 items\n",
            "Class distribution verification:\n",
            "  bird: A=17 (29.3%), B=41 (70.7%)\n",
            "  cat: A=16 (28.6%), B=40 (71.4%)\n",
            "  deer: A=13 (28.9%), B=32 (71.1%)\n",
            "  dog: A=64 (29.9%), B=150 (70.1%)\n",
            "  lynx: A=51 (29.7%), B=121 (70.3%)\n",
            "  opossum: A=4 (25.0%), B=12 (75.0%)\n",
            "  rabbit: A=29 (29.3%), B=70 (70.7%)\n",
            "  skunk: A=6 (28.6%), B=15 (71.4%)\n",
            "  squirrel: A=5 (29.4%), B=12 (70.6%)\n",
            "Processing Set A: 100% 205/205 [00:14<00:00, 13.71it/s]\n",
            "Processing Set B: 100% 493/493 [00:35<00:00, 13.87it/s]\n",
            "Set A (30%): 205 samples, accuracy: 0.8293\n",
            "Set B (70%): 493 samples, accuracy: 0.8296\n",
            "Score A: 0.8292682926829268, Score B: 0.8296146044624746\n",
            "Score saved to score.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cB4iE_BbN8TL"
      },
      "id": "cB4iE_BbN8TL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fbaa9657e2444c5a88b10ca85411868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_7f9b3d56da98483894f97957cfdf1012"
          }
        },
        "c4a1544ce27841a3a648c5b58d6f9ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99015ad0ff6c4bfbae0645d6822bc206",
            "placeholder": "​",
            "style": "IPY_MODEL_b36b28c791984ad29ce1ae26eabb17f2",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "f8ea46fec3b848d4bc1ed271380b4212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1867f60c553e4091938eb27cbc7b087c",
            "placeholder": "​",
            "style": "IPY_MODEL_b2f014664d5b43db8f571bdeb53614c1",
            "value": ""
          }
        },
        "ffb05799106949c4a9b75fc4f1bb3e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3d45f7b95eef49738822743876b4dbfc",
            "style": "IPY_MODEL_741e99269f024bcabc9fcb758acc7c73",
            "value": true
          }
        },
        "8d9d1ee67901447c99b4f6c90c279880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_289693679d8c45f19cc4c14af57a12a9",
            "style": "IPY_MODEL_5291ff294c2e46719e7cf7ad8da30fe4",
            "tooltip": ""
          }
        },
        "cb28ffc900d4416691fc1ade434b0622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0543e7d64de4d3c8289e50bb297c874",
            "placeholder": "​",
            "style": "IPY_MODEL_a9d4c76566404b788ebf36fa528b97c0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7f9b3d56da98483894f97957cfdf1012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "99015ad0ff6c4bfbae0645d6822bc206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b36b28c791984ad29ce1ae26eabb17f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1867f60c553e4091938eb27cbc7b087c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f014664d5b43db8f571bdeb53614c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d45f7b95eef49738822743876b4dbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741e99269f024bcabc9fcb758acc7c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "289693679d8c45f19cc4c14af57a12a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5291ff294c2e46719e7cf7ad8da30fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a0543e7d64de4d3c8289e50bb297c874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d4c76566404b788ebf36fa528b97c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d64d55a004e4854ac6b46137ccdccc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb6b0b3477d45bd9cf0259258eaedf3",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0d8a36c8594ee899276f1d3be44e13",
            "value": "Connecting..."
          }
        },
        "abb6b0b3477d45bd9cf0259258eaedf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0d8a36c8594ee899276f1d3be44e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}