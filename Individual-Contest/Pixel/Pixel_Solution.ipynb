{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "441c46a0-b20a-4dc1-9227-f659833a7d2f",
      "metadata": {
        "id": "441c46a0-b20a-4dc1-9227-f659833a7d2f"
      },
      "source": [
        "# Pixel Efficiency: Reference Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5d573331",
      "metadata": {
        "id": "5d573331",
        "outputId": "f1742581-1706-403f-b598-0d64612663a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install numpy pillow tqdm torch transformers datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "BzUHK_vUquxS",
        "outputId": "25cb6eac-cd0b-441e-d0c1-343271dfc070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "8eec8e6bfa704871bd889d23c22384cd",
            "d34303132ede415ea4f7ac5e335572d2",
            "7f34f314dea8429fbb6b374723f23db2",
            "db4b7385b7c04309a14c4e232204cb8e",
            "1317b5bc90b748fbbcbc0c420b977743",
            "8da049953df64134b6ac2777d2eb646a",
            "6d62022c0dbb4a00997edda428c1a492",
            "6b60c7c9183e4ef194993b0216fe8a0e",
            "1fe59a4f08f24066a2bdb5399e51c048",
            "dc360f79959d458f8c605753bba5cd75",
            "4b5b55c6cb3f4acc971371fdf4ddfc2d",
            "ed8d3269c2004416bbab2de36fca384d",
            "69360b6496ff4b509a630dba4d99bbcb",
            "ac9571233a9d41358dbc8a138c5323a3",
            "ca741307ccc9426e8c8a8c64bb053839",
            "4a734911eaf54510b12c49a64be3e6d5",
            "26df25a1a24f42eabc467f3da98be0e6"
          ]
        }
      },
      "id": "BzUHK_vUquxS",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8eec8e6bfa704871bd889d23c22384cd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df5ae63",
      "metadata": {
        "id": "2df5ae63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from typing import Optional\n",
        "from transformers.models.clip.modeling_clip import CLIPVisionTransformer, CLIPVisionConfig, BaseModelOutputWithPooling\n",
        "\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_PATH = \"IOAI-official/IOAI-2025-Pixel-test\"\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_PATH = \"openai/clip-vit-large-patch14\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BACKGROUND_CLASS = \"other\"\n",
        "\n",
        "# Image and Masking Configuration\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "RETAIN_RATIO = 0.0625\n",
        "MEAN_COLOR = (0, 0, 0)\n",
        "STRIDE = 2\n",
        "TOP_K = 3\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(DATASET_PATH, split=SPLIT)\n",
        "\n",
        "print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n",
        "\n",
        "print(f\"Loading CLIP model and processor: {MODEL_PATH}...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_PATH).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_PATH)\n",
        "print(\"Model and processor loaded successfully.\")\n",
        "\n",
        "\n",
        "def generate_all_rectangular_regions(image_size=224, patch_size=14, retain_ratio=RETAIN_RATIO, max_aspect_ratio=1.2, stride=1):\n",
        "    \"\"\"Generate rectangular regions with optimizations for speed\"\"\"\n",
        "    max_pixels = int(retain_ratio * image_size * image_size)\n",
        "    patches_per_side = image_size // patch_size\n",
        "    patch_area = patch_size * patch_size\n",
        "    target_patches = max_pixels // patch_area\n",
        "\n",
        "    min_patches = max(1, target_patches - 1)\n",
        "    max_patches = target_patches + 1\n",
        "\n",
        "    regions = []\n",
        "    region_to_patches = []\n",
        "\n",
        "    # Pre-compute valid rectangle dimensions\n",
        "    valid_dims = []\n",
        "    for width_patches in range(1, patches_per_side + 1):\n",
        "        for height_patches in range(1, patches_per_side + 1):\n",
        "            total_patches = width_patches * height_patches\n",
        "            if min_patches <= total_patches <= max_patches:\n",
        "                aspect_ratio = max(width_patches, height_patches) / min(width_patches, height_patches)\n",
        "                if aspect_ratio <= max_aspect_ratio:\n",
        "                    valid_dims.append((width_patches, height_patches, total_patches))\n",
        "\n",
        "    # Generate rectangles using stride for positions\n",
        "    for width_patches, height_patches, total_patches in valid_dims:\n",
        "        for top_patch in range(0, patches_per_side - height_patches + 1, stride):\n",
        "            for left_patch in range(0, patches_per_side - width_patches + 1, stride):\n",
        "                bottom_patch = top_patch + height_patches\n",
        "                right_patch = left_patch + width_patches\n",
        "\n",
        "                pixel_coords = (\n",
        "                    top_patch * patch_size,\n",
        "                    left_patch * patch_size,\n",
        "                    bottom_patch * patch_size,\n",
        "                    right_patch * patch_size\n",
        "                )\n",
        "                regions.append(pixel_coords)\n",
        "\n",
        "                covered_patches = []\n",
        "                for p_row in range(top_patch, bottom_patch):\n",
        "                    for p_col in range(left_patch, right_patch):\n",
        "                        patch_idx = p_row * patches_per_side + p_col\n",
        "                        covered_patches.append(patch_idx)\n",
        "                region_to_patches.append(covered_patches)\n",
        "\n",
        "    return regions, region_to_patches\n",
        "\n",
        "\n",
        "class MaskCLIPVisionTransformer(CLIPVisionTransformer):\n",
        "    \"\"\"Modified CLIP Vision Transformer that supports mask tokens for all possible rectangular regions\"\"\"\n",
        "\n",
        "    def __init__(self, config: CLIPVisionConfig, retain_ratio=RETAIN_RATIO):\n",
        "        super().__init__(config)\n",
        "        self.retain_ratio = retain_ratio\n",
        "        self.num_patches = (config.image_size // config.patch_size) ** 2\n",
        "\n",
        "        self.regions, self.region_to_patches = generate_all_rectangular_regions(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            retain_ratio=retain_ratio,\n",
        "            max_aspect_ratio=1.2,\n",
        "            stride=STRIDE\n",
        "        )\n",
        "        self.num_mask_tokens = len(self.regions)\n",
        "\n",
        "        self.mask_tokens = nn.Parameter(torch.randn(1, self.num_mask_tokens, config.hidden_size))\n",
        "\n",
        "    def create_mask_attention_matrix(self, batch_size):\n",
        "        \"\"\"Create attention mask matrix for all rectangular regions\"\"\"\n",
        "        N = self.num_patches\n",
        "        M = self.num_mask_tokens\n",
        "        total_tokens = N + 1 + M\n",
        "\n",
        "        attention_mask = torch.zeros(total_tokens, total_tokens, dtype=torch.bool, device=self.mask_tokens.device)\n",
        "\n",
        "        # Class token and image patches do NOT attend to mask tokens\n",
        "        attention_mask[:N+1, N+1:] = True\n",
        "\n",
        "        # Each mask token attends to its specific image patches (not CLS)\n",
        "        attention_mask[N+1:, 1:N+1] = True\n",
        "\n",
        "        # Then allow each mask token to attend to its assigned patches\n",
        "        for mask_idx in range(M):\n",
        "            covered_patches = self.region_to_patches[mask_idx]\n",
        "            for patch_idx in covered_patches:\n",
        "                token_pos = 1 + patch_idx\n",
        "                attention_mask[N + 1 + mask_idx, token_pos] = False\n",
        "\n",
        "        # Mask tokens do NOT attend to each other\n",
        "        attention_mask[N+1:, N+1:] = True\n",
        "        # Allow self-attention for each mask token\n",
        "        for i in range(M):\n",
        "            attention_mask[N + 1 + i, N + 1 + i] = False\n",
        "\n",
        "        return attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        interpolate_pos_encoding: Optional[bool] = False,\n",
        "        use_mask_tokens: bool = False,\n",
        "    ) -> BaseModelOutputWithPooling:\n",
        "        \"\"\"Forward pass with optional mask tokens\"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Get embeddings (patches + class token)\n",
        "        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
        "        hidden_states = self.pre_layrnorm(hidden_states)\n",
        "\n",
        "        if use_mask_tokens:\n",
        "            # Add mask tokens to the sequence\n",
        "            batch_size = hidden_states.shape[0]\n",
        "\n",
        "            cls_token_embedding = hidden_states[:, 0:1, :]\n",
        "            mask_tokens_expanded = cls_token_embedding.expand(batch_size, self.num_mask_tokens, -1)\n",
        "\n",
        "            if mask_tokens_expanded.device != hidden_states.device:\n",
        "                mask_tokens_expanded = mask_tokens_expanded.to(hidden_states.device)\n",
        "\n",
        "            hidden_states = torch.cat([hidden_states, mask_tokens_expanded], dim=1)\n",
        "\n",
        "            # Create custom attention mask\n",
        "            attention_mask = self.create_mask_attention_matrix(batch_size)\n",
        "\n",
        "            seq_len = hidden_states.shape[1]\n",
        "            attention_mask_4d = attention_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1, -1)\n",
        "            attention_mask_4d = attention_mask_4d.float()\n",
        "            attention_mask_4d = attention_mask_4d.masked_fill(attention_mask_4d == 1, float('-inf'))\n",
        "            attention_mask_4d = attention_mask_4d.masked_fill(attention_mask_4d == 0, 0.0)\n",
        "        else:\n",
        "            attention_mask_4d = None\n",
        "\n",
        "        # Process through encoder layers\n",
        "        encoder_outputs = self.encoder(\n",
        "            inputs_embeds=hidden_states,\n",
        "            attention_mask=attention_mask_4d,\n",
        "            causal_attention_mask=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = encoder_outputs.last_hidden_state\n",
        "\n",
        "        if use_mask_tokens:\n",
        "            # Extract different token types\n",
        "            class_token_output = last_hidden_state[:, 0]\n",
        "            mask_tokens_output = last_hidden_state[:, self.num_patches + 1:]\n",
        "\n",
        "            # Apply post layer norm\n",
        "            pooled_output = self.post_layernorm(class_token_output)\n",
        "            mask_tokens_output = self.post_layernorm(mask_tokens_output)\n",
        "\n",
        "            return {\n",
        "                'last_hidden_state': last_hidden_state,\n",
        "                'pooler_output': pooled_output,\n",
        "                'mask_tokens_output': mask_tokens_output,\n",
        "                'hidden_states': encoder_outputs.hidden_states,\n",
        "                'attentions': encoder_outputs.attentions,\n",
        "            }\n",
        "        else:\n",
        "            # Standard CLIP behavior\n",
        "            pooled_output = last_hidden_state[:, 0, :]\n",
        "            pooled_output = self.post_layernorm(pooled_output)\n",
        "\n",
        "            return BaseModelOutputWithPooling(\n",
        "                last_hidden_state=last_hidden_state,\n",
        "                pooler_output=pooled_output,\n",
        "                hidden_states=encoder_outputs.hidden_states,\n",
        "                attentions=encoder_outputs.attentions,\n",
        "            )\n",
        "\n",
        "\n",
        "def apply_mask_with_mean(image, mask, mean_rgb=MEAN_COLOR):\n",
        "    \"\"\"Apply arbitrary binary mask to image, replacing masked areas with mean values\"\"\"\n",
        "    img_array = np.array(image).copy()\n",
        "\n",
        "    if isinstance(mask, Image.Image):\n",
        "        mask_array = np.array(mask.convert('L')) > 127\n",
        "    else:\n",
        "        mask_array = mask > 0\n",
        "\n",
        "    mask_3d = np.stack([mask_array] * 3, axis=2)\n",
        "    mean_values = np.array([int(m * 255) for m in mean_rgb])\n",
        "    img_array = np.where(mask_3d, img_array, mean_values.reshape(1, 1, 3))\n",
        "\n",
        "    return Image.fromarray(img_array.astype(np.uint8))\n",
        "\n",
        "\n",
        "def compute_vision_features_once(model, image, mask_vision_model):\n",
        "    \"\"\"\n",
        "    Compute vision features once for efficient reuse across multiple mask selection functions.\n",
        "    This eliminates redundant forward passes.\n",
        "    \"\"\"\n",
        "    image_inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vision_outputs = mask_vision_model(\n",
        "            pixel_values=image_inputs['pixel_values'],\n",
        "            use_mask_tokens=True\n",
        "        )\n",
        "\n",
        "        full_image_features = vision_outputs['pooler_output']\n",
        "        if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "            full_image_features = model.visual_projection(full_image_features)\n",
        "\n",
        "        mask_tokens_features = vision_outputs['mask_tokens_output']\n",
        "        if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "            batch_size, num_tokens, embed_dim = mask_tokens_features.shape\n",
        "            mask_tokens_features = mask_tokens_features.view(-1, embed_dim)\n",
        "            mask_tokens_features = model.visual_projection(mask_tokens_features)\n",
        "            mask_tokens_features = mask_tokens_features.view(batch_size, num_tokens, -1)\n",
        "\n",
        "        full_image_features = full_image_features / full_image_features.norm(dim=-1, keepdim=True)\n",
        "        mask_tokens_features = mask_tokens_features / mask_tokens_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    return vision_outputs, full_image_features, mask_tokens_features\n",
        "\n",
        "\n",
        "def find_best_mask_region_calibrated(model, image, class_names, mask_vision_model, text_features,\n",
        "                                   vision_outputs=None, full_image_features=None, mask_tokens_features=None,\n",
        "                                   return_detailed=False):\n",
        "    \"\"\"Find the best mask region using MaskCLIP approach with calibration for black-pixel masking\"\"\"\n",
        "    num_mask_tokens = mask_vision_model.num_mask_tokens\n",
        "\n",
        "    # Use pre-computed features if provided, otherwise compute them\n",
        "    if vision_outputs is None or full_image_features is None or mask_tokens_features is None:\n",
        "        image_inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vision_outputs = mask_vision_model(\n",
        "                pixel_values=image_inputs['pixel_values'],\n",
        "                use_mask_tokens=True\n",
        "            )\n",
        "\n",
        "            full_image_features = vision_outputs['pooler_output']\n",
        "            if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "                full_image_features = model.visual_projection(full_image_features)\n",
        "\n",
        "            mask_tokens_features = vision_outputs['mask_tokens_output']\n",
        "            if hasattr(model, 'visual_projection') and model.visual_projection is not None:\n",
        "                batch_size, num_tokens, embed_dim = mask_tokens_features.shape\n",
        "                mask_tokens_features = mask_tokens_features.view(-1, embed_dim)\n",
        "                mask_tokens_features = model.visual_projection(mask_tokens_features)\n",
        "                mask_tokens_features = mask_tokens_features.view(batch_size, num_tokens, -1)\n",
        "\n",
        "            full_image_features = full_image_features / full_image_features.norm(dim=-1, keepdim=True)\n",
        "            mask_tokens_features = mask_tokens_features / mask_tokens_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute similarity between full image and text\n",
        "    full_image_similarities = torch.matmul(full_image_features, text_features.T)\n",
        "    full_image_prediction = torch.argmax(full_image_similarities, dim=-1)\n",
        "    predicted_class_idx = full_image_prediction.item()\n",
        "\n",
        "    # Compute similarities for each mask token\n",
        "    mask_similarities = torch.matmul(mask_tokens_features.squeeze(0), text_features.T)\n",
        "    mask_predictions = torch.argmax(mask_similarities, dim=-1)\n",
        "\n",
        "    # Get candidates that predict the same class as full image, sorted by confidence\n",
        "    matching_masks = (mask_predictions == predicted_class_idx)\n",
        "\n",
        "    if matching_masks.any():\n",
        "        candidate_indices = torch.where(matching_masks)[0]\n",
        "        candidate_confidences = mask_similarities[candidate_indices, predicted_class_idx]\n",
        "        sorted_indices = torch.argsort(candidate_confidences, descending=True)\n",
        "        sorted_candidates = candidate_indices[sorted_indices]\n",
        "    else:\n",
        "        # If no exact matches, use all candidates sorted by confidence for predicted class\n",
        "        candidate_confidences = mask_similarities[:, predicted_class_idx]\n",
        "        sorted_candidates = torch.topk(candidate_confidences, len(candidate_confidences)).indices\n",
        "\n",
        "    # OPTIMIZATION: If TOP_K=1, skip calibration and return best candidate directly\n",
        "    if TOP_K == 1:\n",
        "        return sorted_candidates[0].item()\n",
        "\n",
        "    # CALIBRATION STEP: Test top K candidates, return immediately when one is correct\n",
        "    calibration_results = []\n",
        "    candidates_to_test = sorted_candidates[:TOP_K]\n",
        "\n",
        "    for i, candidate_idx in enumerate(candidates_to_test):\n",
        "        candidate_idx_item = candidate_idx.item()\n",
        "\n",
        "        # Create masked image for this candidate\n",
        "        coordinates = mask_idx_to_coordinates(candidate_idx_item, mask_vision_model)\n",
        "        mask = generate_mask_from_coordinates(image, coordinates)\n",
        "        masked_image = apply_mask_with_mean(image, mask)\n",
        "\n",
        "        # Test with actual forward pass\n",
        "        with torch.no_grad():\n",
        "            masked_image_inputs = processor(images=masked_image, return_tensors=\"pt\").to(DEVICE)\n",
        "            masked_image_features = model.get_image_features(**masked_image_inputs)\n",
        "            masked_image_features = masked_image_features / masked_image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            masked_similarities = torch.matmul(masked_image_features, text_features.T)\n",
        "            masked_prediction = torch.argmax(masked_similarities, dim=-1).item()\n",
        "            masked_confidence = masked_similarities[0, predicted_class_idx].item()\n",
        "\n",
        "        # If this candidate predicts correctly, return it immediately (early exit optimization)\n",
        "        if masked_prediction == predicted_class_idx:\n",
        "            return candidate_idx_item\n",
        "\n",
        "        # Store failed calibration result\n",
        "        calibration_results.append((candidate_idx_item, masked_confidence))\n",
        "\n",
        "    # If we reach here, all TOP_K candidates failed calibration\n",
        "    # Fall back to the next best candidate from sorted list WITHOUT additional calibration\n",
        "    if len(sorted_candidates) > TOP_K:\n",
        "        return sorted_candidates[TOP_K].item()\n",
        "    else:\n",
        "        # If no more candidates available, return the best failed calibration result\n",
        "        if calibration_results:\n",
        "            return max(calibration_results, key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            # Ultimate fallback: return the best mask token prediction\n",
        "            return sorted_candidates[0].item()\n",
        "\n",
        "\n",
        "def mask_idx_to_coordinates(mask_idx, mask_vision_model):\n",
        "    \"\"\"Convert mask token index to image coordinates using the pre-computed regions\"\"\"\n",
        "    if mask_idx >= len(mask_vision_model.regions):\n",
        "        raise ValueError(f\"mask_idx {mask_idx} is out of range. Only {len(mask_vision_model.regions)} regions available.\")\n",
        "\n",
        "    top, left, bottom, right = mask_vision_model.regions[mask_idx]\n",
        "    return ((top, left), (bottom, right))\n",
        "\n",
        "\n",
        "def generate_mask_from_coordinates(image, coordinates):\n",
        "    \"\"\"Generate a binary mask from crop coordinates\"\"\"\n",
        "    H, W = 224, 224\n",
        "    mask = np.zeros((H, W), dtype=np.int8)\n",
        "\n",
        "    (top, left), (bottom, right) = coordinates\n",
        "    mask[top:bottom, left:right] = 1\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# Create the MaskCLIP model\n",
        "print(\"Creating MaskCLIP model...\")\n",
        "mask_vision_model = MaskCLIPVisionTransformer(model.vision_model.config, retain_ratio=RETAIN_RATIO)\n",
        "mask_vision_model.load_state_dict(model.vision_model.state_dict(), strict=False)\n",
        "mask_vision_model = mask_vision_model.to(DEVICE)\n",
        "mask_vision_model.eval()\n",
        "print(\"MaskCLIP model created successfully.\")\n",
        "\n",
        "# Get class names from training dataset for consistent evaluation\n",
        "train_dataset = load_dataset(\"IOAI-official/IOAI-2025-Pixel-train\", split=\"train\")\n",
        "class_names_eval = list(set([item['name'] for item in train_dataset])) + [BACKGROUND_CLASS]\n",
        "\n",
        "# Prepare text features once for efficiency\n",
        "print(\"Preparing text features...\")\n",
        "text_inputs_eval = processor(text=class_names_eval, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    text_features_eval = model.get_text_features(**text_inputs_eval)\n",
        "    text_features_eval = text_features_eval / text_features_eval.norm(dim=-1, keepdim=True)\n",
        "print(\"Text features prepared.\")\n",
        "\n",
        "# Main evaluation loop\n",
        "masks = {}\n",
        "total_correct = 0\n",
        "total_processed = 0\n",
        "\n",
        "for item in tqdm(dataset):\n",
        "    image = item['image']\n",
        "    total_processed += 1\n",
        "\n",
        "    try:\n",
        "        # Compute vision features once for efficiency (eliminates redundant forward passes)\n",
        "        vision_outputs, full_image_features, mask_tokens_features = compute_vision_features_once(\n",
        "            model, image, mask_vision_model\n",
        "        )\n",
        "\n",
        "        # Get prediction from pre-computed features\n",
        "        full_image_similarities = torch.matmul(full_image_features, text_features_eval.T)\n",
        "        predicted_class_idx = torch.argmax(full_image_similarities, dim=-1).item()\n",
        "\n",
        "        best_mask_idx = find_best_mask_region_calibrated(\n",
        "            model, image, class_names_eval, mask_vision_model, text_features_eval,\n",
        "            vision_outputs=vision_outputs, full_image_features=full_image_features,\n",
        "            mask_tokens_features=mask_tokens_features\n",
        "        )\n",
        "\n",
        "        coordinates = mask_idx_to_coordinates(best_mask_idx, mask_vision_model)\n",
        "\n",
        "        # Validate the mask\n",
        "        mask = generate_mask_from_coordinates(image, coordinates)\n",
        "        assert mask.shape == (224, 224), \"Mask should be 224x224\"\n",
        "        assert mask.sum() <= RETAIN_RATIO * 224 * 224, \"You should leave only 6.25% of pixels\"\n",
        "\n",
        "\n",
        "        # Save the coordinates\n",
        "        idx = item['idx']\n",
        "        masks[idx] = coordinates\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {item['idx']}: {e}\")\n",
        "        # Fallback to a small center region if there's an error\n",
        "        if len(mask_vision_model.regions) > 0:\n",
        "            region_sizes = [(r[2]-r[0])*(r[3]-r[1]) for r in mask_vision_model.regions]\n",
        "            min_region_idx = region_sizes.index(min(region_sizes))\n",
        "            fallback_coords = mask_idx_to_coordinates(min_region_idx, mask_vision_model)\n",
        "        else:\n",
        "            fallback_coords = ((84, 84), (140, 140))\n",
        "        masks[item['idx']] = fallback_coords\n",
        "\n",
        "# Save as JSONL (one JSON object per line) - much safer than pickle\n",
        "with open('submission.jsonl', 'w') as f:\n",
        "    for idx, coordinates in masks.items():\n",
        "        json.dump({\"idx\": idx, \"coordinates\": coordinates}, f)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"Masks saved to masks.jsonl\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8eec8e6bfa704871bd889d23c22384cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d34303132ede415ea4f7ac5e335572d2",
              "IPY_MODEL_7f34f314dea8429fbb6b374723f23db2",
              "IPY_MODEL_db4b7385b7c04309a14c4e232204cb8e",
              "IPY_MODEL_1317b5bc90b748fbbcbc0c420b977743",
              "IPY_MODEL_8da049953df64134b6ac2777d2eb646a"
            ],
            "layout": "IPY_MODEL_6d62022c0dbb4a00997edda428c1a492"
          }
        },
        "d34303132ede415ea4f7ac5e335572d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b60c7c9183e4ef194993b0216fe8a0e",
            "placeholder": "​",
            "style": "IPY_MODEL_1fe59a4f08f24066a2bdb5399e51c048",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "7f34f314dea8429fbb6b374723f23db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dc360f79959d458f8c605753bba5cd75",
            "placeholder": "​",
            "style": "IPY_MODEL_4b5b55c6cb3f4acc971371fdf4ddfc2d",
            "value": ""
          }
        },
        "db4b7385b7c04309a14c4e232204cb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ed8d3269c2004416bbab2de36fca384d",
            "style": "IPY_MODEL_69360b6496ff4b509a630dba4d99bbcb",
            "value": true
          }
        },
        "1317b5bc90b748fbbcbc0c420b977743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ac9571233a9d41358dbc8a138c5323a3",
            "style": "IPY_MODEL_ca741307ccc9426e8c8a8c64bb053839",
            "tooltip": ""
          }
        },
        "8da049953df64134b6ac2777d2eb646a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a734911eaf54510b12c49a64be3e6d5",
            "placeholder": "​",
            "style": "IPY_MODEL_26df25a1a24f42eabc467f3da98be0e6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "6d62022c0dbb4a00997edda428c1a492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6b60c7c9183e4ef194993b0216fe8a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe59a4f08f24066a2bdb5399e51c048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc360f79959d458f8c605753bba5cd75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5b55c6cb3f4acc971371fdf4ddfc2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed8d3269c2004416bbab2de36fca384d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69360b6496ff4b509a630dba4d99bbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac9571233a9d41358dbc8a138c5323a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca741307ccc9426e8c8a8c64bb053839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4a734911eaf54510b12c49a64be3e6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26df25a1a24f42eabc467f3da98be0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}